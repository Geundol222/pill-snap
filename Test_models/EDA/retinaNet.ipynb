{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAJJRuMUCitD"
   },
   "source": [
    "## 섹터 0 & 1: 기본 설정 (환경, 경로, 하이퍼파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27485,
     "status": "ok",
     "timestamp": 1757838496398,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "qrOvHxY9Brog",
    "outputId": "5d94e7f6-6904-4d30-c666-1c08cadc4c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\n",
      "[INFO] 프로젝트 경로: /content/drive/MyDrive/datasets/pills\n",
      "[INFO] 학습 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 0 & 1] 환경 준비 및 기본 설정 (오류 수정)\n",
    "# ================================================================\n",
    "\n",
    "# --- 환경 설정 ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- 기본 라이브러리 임포트 ---\n",
    "import torch  # <--- 이 줄이 추가되었습니다.\n",
    "import os, json, random, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # 손상된 이미지도 최대한 로드\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "# 1. 프로젝트 최상위 폴더 (수정 필요 시)\n",
    "ROOT = Path(\"/content/drive/MyDrive/datasets/pills\")\n",
    "\n",
    "# 2. 하위 폴더 및 파일 경로 (자동 설정)\n",
    "IMG_DIR = ROOT / \"train_images\"\n",
    "ANN_DIR = ROOT / \"train_annotations\"      # 원본 Annotation 폴더\n",
    "OUTPUT_DIR = ROOT / \"outputs\"             # EDA 결과물이 저장된 폴더\n",
    "\n",
    "# EDA에서 생성한 데이터 분할 파일을 사용하도록 경로 변경\n",
    "SPLITS_PATH = OUTPUT_DIR / \"RetinaNet_splits.json\"\n",
    "MERGED_JSON = OUTPUT_DIR / \"RetinaNet_coco_merged.json\" # 통합 Annotation 파일 저장 경로\n",
    "CKPT_DIR = ROOT / \"checkpoints\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 하이퍼파라미터 ---\n",
    "SEED = 42\n",
    "BATCH_SIZE = 4      # GPU 메모리 부족(OOM) 시 2로 줄이기\n",
    "NUM_WORKERS = 2     # Colab 기본값\n",
    "EPOCHS = 50         # 전체 학습 에폭 수\n",
    "LR = 0.005          # 학습률 (Learning Rate)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = True      # 혼합 정밀도 사용 (메모리 절약, 속도 향상)\n",
    "\n",
    "# 시드 고정 (실험 재현성 확보)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- 경로 존재 여부 확인 ---\n",
    "assert IMG_DIR.exists(), f\"이미지 폴더가 없습니다: {IMG_DIR}\"\n",
    "assert ANN_DIR.exists(), f\"Annotation 폴더가 없습니다: {ANN_DIR}\"\n",
    "print(f\"\\n[INFO] 프로젝트 경로: {ROOT}\")\n",
    "print(f\"[INFO] 학습 장치: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGHWYBPYC4BM"
   },
   "source": [
    "## 섹터 2: Annotation 파일 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1757838497189,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "s6cqFIZBC7Gr",
    "outputId": "80806b56-c1d0-40b9-b027-650e09b0ead0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 캐시된 통합 Annotation 파일 사용: /content/drive/MyDrive/datasets/pills/outputs/RetinaNet_coco_merged.json\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 2] Annotation 파일 병합 (없을 경우에만 실행) - (수정)\n",
    "# ================================================================\n",
    "# 설명: 여러 JSON 파일을 하나로 합치면서, 모든 이미지와 Annotation에\n",
    "#      고유한 ID를 새로 부여하여 데이터 충돌을 방지합니다.\n",
    "\n",
    "FORCE_REBUILD = False # True로 설정 시 항상 새로 병합\n",
    "\n",
    "if MERGED_JSON.exists() and not FORCE_REBUILD:\n",
    "    with open(MERGED_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        COCO = json.load(f)\n",
    "    print(f\"[INFO] 캐시된 통합 Annotation 파일 사용: {MERGED_JSON}\")\n",
    "else:\n",
    "    print(f\"[INFO] 통합 Annotation 파일({MERGED_JSON})이 없어 새로 생성합니다.\")\n",
    "\n",
    "    # --- (추가) 실제 파일 병합 로직 ---\n",
    "    # 원본 Annotation 폴더에서 모든 .json 파일을 찾음\n",
    "    json_files = [p for p in ANN_DIR.rglob(\"*.json\")]\n",
    "    assert json_files, f\"Annotation 파일을 찾을 수 없습니다: {ANN_DIR}\"\n",
    "\n",
    "    # 병합된 데이터를 담을 컨테이너\n",
    "    new_images = []\n",
    "    new_annotations = []\n",
    "    category_names = set() # 중복 없는 카테고리 이름을 저장\n",
    "\n",
    "    # ID를 새로 부여하기 위한 카운터\n",
    "    next_img_id = 1\n",
    "    next_ann_id = 1\n",
    "\n",
    "    for json_path in json_files:\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"경고: {json_path} 파일 읽기 실패. 건너뜁니다. ({e})\")\n",
    "            continue\n",
    "\n",
    "        # 각 파일의 이미지 ID를 전역 ID로 매핑하기 위한 딕셔너리\n",
    "        old_to_new_img_id = {}\n",
    "\n",
    "        # 이미지 정보 병합\n",
    "        for img in data.get('images', []):\n",
    "            file_name = Path(img['file_name']).name\n",
    "            new_img = {\n",
    "                'id': next_img_id,\n",
    "                'file_name': file_name,\n",
    "                'width': img['width'],\n",
    "                'height': img['height']\n",
    "            }\n",
    "            new_images.append(new_img)\n",
    "            old_to_new_img_id[img['id']] = next_img_id\n",
    "            next_img_id += 1\n",
    "\n",
    "        # 카테고리 이름 수집\n",
    "        local_cats = {c['id']: c['name'] for c in data.get('categories', [])}\n",
    "        for name in local_cats.values():\n",
    "            category_names.add(name)\n",
    "\n",
    "        # Annotation 정보 병합\n",
    "        for ann in data.get('annotations', []):\n",
    "            # 유효하지 않은 Annotation은 건너뜀\n",
    "            if 'bbox' not in ann or ann['image_id'] not in old_to_new_img_id:\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = ann['bbox']\n",
    "            if not (w > 0 and h > 0):\n",
    "                continue\n",
    "\n",
    "            new_ann = {\n",
    "                'id': next_ann_id,\n",
    "                'image_id': old_to_new_img_id[ann['image_id']],\n",
    "                'category_name': local_cats.get(ann['category_id']), # 임시로 이름 저장\n",
    "                'bbox': ann['bbox'],\n",
    "                'iscrowd': ann.get('iscrowd', 0),\n",
    "                'area': w * h\n",
    "            }\n",
    "            new_annotations.append(new_ann)\n",
    "            next_ann_id += 1\n",
    "\n",
    "    # 전체 카테고리 이름에 대해 고유 ID 부여\n",
    "    sorted_names = sorted(list(category_names))\n",
    "    name_to_cat_id = {name: i + 1 for i, name in enumerate(sorted_names)}\n",
    "\n",
    "    new_categories = [{'id': cat_id, 'name': name} for name, cat_id in name_to_cat_id.items()]\n",
    "\n",
    "    # Annotation에 최종 category_id 할당\n",
    "    for ann in new_annotations:\n",
    "        ann['category_id'] = name_to_cat_id[ann['category_name']]\n",
    "        del ann['category_name'] # 임시 키 제거\n",
    "\n",
    "    # 최종 COCO 객체 생성\n",
    "    COCO = {\n",
    "        \"images\": new_images,\n",
    "        \"annotations\": new_annotations,\n",
    "        \"categories\": new_categories\n",
    "    }\n",
    "\n",
    "    # 파일로 저장하여 다음 실행 시 재사용\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    with open(MERGED_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(COCO, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[SUCCESS] 파일 병합 완료 및 저장: {MERGED_JSON}\")\n",
    "    print(f\"  -> 이미지: {len(COCO['images'])}개, Annotation: {len(COCO['annotations'])}개, 클래스: {len(COCO['categories'])}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8etuqPLqC9uW"
   },
   "source": [
    "## 섹터 3 & 4: 인덱싱 및 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5109,
     "status": "ok",
     "timestamp": 1757838502297,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "0qCHlLJ0DAA4",
    "outputId": "1fa3bec9-7d14-43cc-c024-06b8ab4f5077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 인덱싱 완료: 이미지 4526개, 클래스 73개\n",
      "[INFO] 데이터 분할 완료: Train 4060개 / Validation 466개\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 3 & 4] 인덱싱 및 데이터 분할\n",
    "# ================================================================\n",
    "\n",
    "# --- 섹터 3: 인덱스 생성 ---\n",
    "img_meta = {im[\"id\"]: im for im in COCO[\"images\"]}\n",
    "ann_by_img = defaultdict(list)\n",
    "for a in COCO[\"annotations\"]:\n",
    "    ann_by_img[a[\"image_id\"]].append(a)\n",
    "id2path = {i: (IMG_DIR / Path(img_meta[i][\"file_name\"]).name) for i in img_meta}\n",
    "K = len(COCO[\"categories\"])\n",
    "print(f\"[INFO] 인덱싱 완료: 이미지 {len(img_meta)}개, 클래스 {K}개\")\n",
    "\n",
    "\n",
    "# --- 섹터 4: 데이터 분할 ---\n",
    "# 설명: EDA 단계에서 생성된 RetinaNet_splits.json 파일을 읽어\n",
    "#      학습용(train_ids)과 검증용(val_ids) 이미지 ID 리스트를 만듭니다.\n",
    "assert SPLITS_PATH.exists(), f\"데이터 분할 파일이 없습니다: {SPLITS_PATH}\"\n",
    "\n",
    "with open(SPLITS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "train_ids = splits['train_ids']\n",
    "val_ids = splits['val_ids']\n",
    "\n",
    "# 실제 파일이 존재하는 ID만 최종 사용\n",
    "train_ids = [i for i in train_ids if id2path[i].exists()]\n",
    "val_ids = [i for i in val_ids if id2path[i].exists()]\n",
    "\n",
    "assert len(train_ids) > 0 and len(val_ids) > 0, \"학습 또는 검증 데이터가 없습니다.\"\n",
    "print(f\"[INFO] 데이터 분할 완료: Train {len(train_ids)}개 / Validation {len(val_ids)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYxk00LnDJ5g"
   },
   "source": [
    "## 섹터 5 & 6: 데이터셋 클래스 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5398,
     "status": "ok",
     "timestamp": 1757838507697,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "TQYv3_NfDJLI",
    "outputId": "43a76c64-f267-41b2-881a-326234acae0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 클래스 정의 완료.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 5 & 6] 전처리 함수 및 Dataset 클래스 정의\n",
    "# ================================================================\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "# --- 섹터 5: 전처리/증강 함수 ---\n",
    "# (이전 답변과 동일. xywh_to_xyxy_and_clip, resize_keep_aspect, hflip_boxes_xyxy)\n",
    "def xywh_to_xyxy_and_clip(bbox, W, H):\n",
    "    x, y, w, h = bbox; x1=max(0.0,x); y1=max(0.0,y); x2=min(float(W),x+w); y2=min(float(H),y+h); return [x1,y1,x2,y2]\n",
    "def resize_keep_aspect(img, boxes, short=800, max_sz=1333):\n",
    "    W,H=img.size; s,l=(H,W) if H<W else (W,H); sc=short/max(1,s)\n",
    "    if round(l*sc)>max_sz: sc=max_sz/max(1,l)\n",
    "    if abs(sc-1.0)<1e-6: return img, boxes\n",
    "    nW,nH=int(round(W*sc)), int(round(H*sc)); img=img.resize((nW,nH),Image.BILINEAR)\n",
    "    if boxes.numel()>0: boxes=boxes*sc\n",
    "    return img, boxes\n",
    "def hflip_boxes_xyxy(boxes, w):\n",
    "    if boxes.numel()==0: return boxes\n",
    "    x1=boxes[:,0].clone(); x2=boxes[:,2].clone(); boxes[:,0]=w-x2; boxes[:,2]=w-x1\n",
    "    return boxes\n",
    "\n",
    "# --- 섹터 6: Dataset 클래스 정의 ---\n",
    "class CocoLikeDetection(Dataset):\n",
    "    def __init__(self, image_ids: list, augment: bool = True):\n",
    "        self.ids = image_ids\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im_id = self.ids[idx]\n",
    "        with Image.open(id2path[im_id]) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            W, H = im.size\n",
    "\n",
    "        # Annotation 로드 및 유효한 Bbox만 필터링\n",
    "        anns = ann_by_img.get(im_id, [])\n",
    "        boxes_xyxy, labels = [], []\n",
    "        for a in anns:\n",
    "            x1, y1, x2, y2 = xywh_to_xyxy_and_clip(a[\"bbox\"], W, H)\n",
    "            if x2 > x1 and y2 > y1: # 너비와 높이가 0보다 큰 유효한 박스만 추가\n",
    "                boxes_xyxy.append([x1, y1, x2, y2])\n",
    "                labels.append(int(a[\"category_id\"])) # 원본 ID는 1...K\n",
    "\n",
    "        boxes = torch.tensor(boxes_xyxy, dtype=torch.float32) if boxes_xyxy else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,))\n",
    "\n",
    "        # RetinaNet은 0-based label을 사용 (0 ~ K-1)\n",
    "        if labels.numel():\n",
    "            labels = labels - 1\n",
    "\n",
    "        # 전처리 및 증강\n",
    "        im, boxes = resize_keep_aspect(im, boxes)\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                im = TF.hflip(im)\n",
    "                boxes = hflip_boxes_xyxy(boxes, im.size[0])\n",
    "            im = TF.adjust_brightness(im, 0.9 + 0.2 * random.random())\n",
    "            im = TF.adjust_contrast(im, 0.9 + 0.2 * random.random())\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([im_id])\n",
    "        }\n",
    "        return to_tensor(im), target\n",
    "\n",
    "def collate(batch):\n",
    "    return list(zip(*batch))\n",
    "\n",
    "print(\"Dataset 클래스 정의 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkfzANG_DMsR"
   },
   "source": [
    "## 섹터 7 & 8: 데이터로더 및 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2531,
     "status": "ok",
     "timestamp": 1757838510230,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "KaSXLbNTDPCz",
    "outputId": "90d80853-5891-41ed-e9a4-8c8605111f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DataLoader 준비 완료: Train 1015 배치 / Validation 117 배치\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 106MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RetinaNet 모델 준비 완료. 총 클래스 수: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2425322579.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 7 & 8] 데이터로더 및 모델 준비\n",
    "# ================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 섹터 7: DataLoader 생성 ---\n",
    "train_ds = CocoLikeDetection(train_ids, augment=True)\n",
    "val_ds   = CocoLikeDetection(val_ids, augment=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate)\n",
    "print(f\"[INFO] DataLoader 준비 완료: Train {len(train_loader)} 배치 / Validation {len(val_loader)} 배치\")\n",
    "\n",
    "# --- 섹터 8: 모델, 옵티마이저, 스케줄러 준비 ---\n",
    "# torchvision 버전에 따라 호환되는 RetinaNet을 자동으로 불러옵니다.\n",
    "try:\n",
    "    from torchvision.models.detection import retinanet_resnet50_fpn_v2 as retinanet_factory\n",
    "except ImportError:\n",
    "    from torchvision.models.detection import retinanet_resnet50_fpn as retinanet_factory\n",
    "\n",
    "# 사전 학습된 Backbone 가중치를 사용하고, Head 부분만 우리 데이터에 맞게 초기화합니다.\n",
    "model = retinanet_factory(weights_backbone=\"DEFAULT\", num_classes=K)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# 옵티마이저 (SGD) 및 학습률 스케줄러 (StepLR)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=LR, momentum=0.9, weight_decay=1e-4)\n",
    "lr_sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# AMP(혼합 정밀도) 스케일러\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "print(f\"[INFO] RetinaNet 모델 준비 완료. 총 클래스 수: {K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1ZPHKa6DSTT"
   },
   "source": [
    "## 섹터 9: 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 19349363,
     "status": "error",
     "timestamp": 1757859651923,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "lDr38i1VDSps",
    "outputId": "3454e21c-d6e9-4d6d-af48-71facf1c4fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] Train Loss: 0.6258 | Val Loss: 0.5670\n",
      "  -> Best model saved with val_loss: 0.5670\n",
      "[Epoch 02] Train Loss: 0.5279 | Val Loss: 0.4847\n",
      "  -> Best model saved with val_loss: 0.4847\n",
      "[Epoch 03] Train Loss: 0.4599 | Val Loss: 0.4912\n",
      "[Epoch 04] Train Loss: 0.4218 | Val Loss: 0.4051\n",
      "  -> Best model saved with val_loss: 0.4051\n",
      "[Epoch 05] Train Loss: 0.4070 | Val Loss: 0.4202\n",
      "[Epoch 06] Train Loss: 0.3338 | Val Loss: 0.3441\n",
      "  -> Best model saved with val_loss: 0.3441\n",
      "[Epoch 07] Train Loss: 0.3224 | Val Loss: 0.3410\n",
      "  -> Best model saved with val_loss: 0.3410\n",
      "[Epoch 08] Train Loss: 0.3178 | Val Loss: 0.3398\n",
      "  -> Best model saved with val_loss: 0.3398\n",
      "[Epoch 09] Train Loss: 0.3145 | Val Loss: 0.3324\n",
      "  -> Best model saved with val_loss: 0.3324\n",
      "[Epoch 10] Train Loss: 0.3111 | Val Loss: 0.3347\n",
      "[Epoch 11] Train Loss: 0.3037 | Val Loss: 0.3275\n",
      "  -> Best model saved with val_loss: 0.3275\n",
      "[Epoch 12] Train Loss: 0.3029 | Val Loss: 0.3267\n",
      "  -> Best model saved with val_loss: 0.3267\n",
      "[Epoch 13] Train Loss: 0.3021 | Val Loss: 0.3257\n",
      "  -> Best model saved with val_loss: 0.3257\n",
      "[Epoch 14] Train Loss: 0.3020 | Val Loss: 0.3256\n",
      "  -> Best model saved with val_loss: 0.3256\n",
      "[Epoch 15] Train Loss: 0.3019 | Val Loss: 0.3254\n",
      "  -> Best model saved with val_loss: 0.3254\n",
      "[Epoch 16] Train Loss: 0.3000 | Val Loss: 0.3252\n",
      "  -> Best model saved with val_loss: 0.3252\n",
      "[Epoch 17] Train Loss: 0.3005 | Val Loss: 0.3251\n",
      "  -> Best model saved with val_loss: 0.3251\n",
      "[Epoch 18] Train Loss: 0.3000 | Val Loss: 0.3251\n",
      "[Epoch 19] Train Loss: 0.3003 | Val Loss: 0.3252\n",
      "[Epoch 20] Train Loss: 0.3002 | Val Loss: 0.3251\n",
      "  -> Best model saved with val_loss: 0.3251\n",
      "[Epoch 21] Train Loss: 0.3001 | Val Loss: 0.3251\n",
      "  -> Best model saved with val_loss: 0.3251\n",
      "[Epoch 22] Train Loss: 0.2999 | Val Loss: 0.3250\n",
      "  -> Best model saved with val_loss: 0.3250\n",
      "[Epoch 23] Train Loss: 0.3003 | Val Loss: 0.3251\n",
      "[Epoch 24] Train Loss: 0.3004 | Val Loss: 0.3250\n",
      "  -> Best model saved with val_loss: 0.3250\n",
      "[Epoch 25] Train Loss: 0.3001 | Val Loss: 0.3250\n",
      "[Epoch 26] Train Loss: 0.3002 | Val Loss: 0.3250\n",
      "  -> Best model saved with val_loss: 0.3250\n",
      "[Epoch 27] Train Loss: 0.2995 | Val Loss: 0.3250\n",
      "  -> Best model saved with val_loss: 0.3250\n",
      "[Epoch 28] Train Loss: 0.3000 | Val Loss: 0.3250\n",
      "  -> Best model saved with val_loss: 0.3250\n",
      "[Epoch 29] Train Loss: 0.3001 | Val Loss: 0.3250\n",
      "  -> Best model saved with val_loss: 0.3250\n",
      "[Epoch 30] Train Loss: 0.2998 | Val Loss: 0.3250\n",
      "[Epoch 31] Train Loss: 0.3002 | Val Loss: 0.3250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3352153817.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mlr_sch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3352153817.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_AMP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/retinanet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# compute the retinanet heads outputs using the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mhead_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;31m# create the set of anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/retinanet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# type: (list[Tensor]) -> dict[str, Tensor]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"cls_logits\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bbox_regression\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/retinanet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mcls_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mcls_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1950\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 9] 학습 루프 (IndexError 수정)\n",
    "# ================================================================\n",
    "import math\n",
    "from torch.amp import autocast\n",
    "\n",
    "# --- 수정된 학습/검증 함수 ---\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, targets in loader:\n",
    "        imgs = [im.to(DEVICE) for im in imgs]\n",
    "        # targets를 GPU로 보냅니다.\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # --- (오류 수정) ---\n",
    "        # 'labels' 텐서의 타입을 torch.long (int64)으로 확실하게 지정합니다.\n",
    "        for t in targets:\n",
    "            t['labels'] = t['labels'].to(torch.long)\n",
    "        # --------------------\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda', enabled=USE_AMP):\n",
    "            loss_dict = model(imgs, targets)\n",
    "            loss = sum(l for l in loss_dict.values())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_loss(loader):\n",
    "    model.train() # loss 계산을 위해 train 모드\n",
    "    total_loss = 0\n",
    "    for imgs, targets in loader:\n",
    "        imgs = [im.to(DEVICE) for im in imgs]\n",
    "        # targets를 GPU로 보냅니다.\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # --- (오류 수정) ---\n",
    "        # 검증 데이터셋의 'labels' 텐서 타입도 동일하게 맞춰줍니다.\n",
    "        for t in targets:\n",
    "            t['labels'] = t['labels'].to(torch.long)\n",
    "        # --------------------\n",
    "\n",
    "        with autocast('cuda', enabled=USE_AMP):\n",
    "            loss_dict = model(imgs, targets)\n",
    "            loss = sum(l for l in loss_dict.values())\n",
    "        total_loss += loss.item()\n",
    "    model.eval() # 다시 eval 모드로\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- 학습 시작 ---\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    val_loss = validate_loss(val_loader)\n",
    "    lr_sch.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch:02d}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), CKPT_DIR / \"best_model.pth\")\n",
    "        print(f\"  -> Best model saved with val_loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# 가장 좋았던 모델의 가중치를 다시 불러옵니다.\n",
    "print(f\"\\n[INFO] 학습 완료. Best Val Loss: {best_val_loss:.4f}\")\n",
    "model.load_state_dict(torch.load(CKPT_DIR / \"best_model.pth\", map_location=DEVICE))\n",
    "print(\"[INFO] Best model weights loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 섹터 9-1: 최고 성능 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1757859825784,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "qGALq-pvnEHY",
    "outputId": "458aece1-1560-4233-ce5c-546a50d8b886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 학습 중단/완료. 마지막 Best Val Loss: 0.3250\n",
      "[INFO] 최고 성능 모델(best_model.pth)을 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 9] 나머지 부분: 최고 성능 모델 불러오기\n",
    "# ================================================================\n",
    "\n",
    "# best_val_loss 변수가 없으면 임시로 0으로 설정합니다.\n",
    "if 'best_val_loss' not in locals():\n",
    "    best_val_loss = 0.0\n",
    "\n",
    "# 학습 루프가 끝난 후, 가장 성능이 좋았던 모델의 가중치를 불러옵니다.\n",
    "print(f\"\\n[INFO] 학습 중단/완료. 마지막 Best Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# 저장된 best_model.pth 파일의 경로\n",
    "best_model_path = CKPT_DIR / \"best_model.pth\"\n",
    "\n",
    "# 파일이 존재하는지 확인 후 모델에 가중치를 로드\n",
    "if best_model_path.exists():\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    print(f\"[INFO] 최고 성능 모델({best_model_path.name})을 성공적으로 불러왔습니다.\")\n",
    "else:\n",
    "    print(f\"[ERROR] best_model.pth 파일을 찾을 수 없습니다! 학습이 정상적으로 진행되었는지 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oM_zTg6DT2d"
   },
   "source": [
    "## 섹터 10: 평가 (mAP) 및 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285639,
     "status": "ok",
     "timestamp": 1757860440228,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "Bv3Mome2DVGo",
    "outputId": "ad7a90b6-36de-4e29-fc72-b19fd786e26d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- mAP 평가 시작 ---\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=3.58s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.06s).\n",
      " Average Precision  (AP) @[ IoU=0.75:0.95 | area=   all | maxDets=100 ] = 0.325\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.341\n",
      " Average Precision  (AP) @[ IoU=0.75:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75:0.95 | area= large | maxDets=100 ] = 0.325\n",
      " Average Recall     (AR) @[ IoU=0.75:0.95 | area=   all | maxDets=  1 ] = 0.914\n",
      " Average Recall     (AR) @[ IoU=0.75:0.95 | area=   all | maxDets= 10 ] = 0.920\n",
      " Average Recall     (AR) @[ IoU=0.75:0.95 | area=   all | maxDets=100 ] = 0.920\n",
      " Average Recall     (AR) @[ IoU=0.75:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.95 | area= large | maxDets=100 ] = 0.920\n",
      "\n",
      "[RESULT] 최종 mAP @[IoU=0.75:0.95] = 0.3251\n",
      "\n",
      "--- 제출 파일 생성 시작 ---\n",
      "\n",
      "[SUBMIT] 제출 파일 생성 완료: /content/drive/MyDrive/datasets/pills/submission.csv\n",
      "  -> 총 44600개의 객체를 843개의 이미지에서 예측했습니다.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 10] 평가(mAP) 및 제출 파일 생성 (KeyError 수정)\n",
    "# ================================================================\n",
    "# pycocotools가 설치되어 있는지 확인, 없으면 설치\n",
    "try:\n",
    "    from pycocotools.coco import COCO as COCOapi\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "except ImportError:\n",
    "    print(\"pycocotools 설치를 시작합니다...\")\n",
    "    !pip install -q pycocotools\n",
    "    from pycocotools.coco import COCO as COCOapi\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "# --- 평가 및 제출용 설정값 ---\n",
    "TEST_DIR = ROOT / \"test_images\"\n",
    "SUBMIT_CSV = ROOT / \"submission.csv\"\n",
    "EVAL_SCORE_THR = 0.05  # mAP 평가 시 예측으로 인정할 최소 점수(confidence)\n",
    "\n",
    "# --- 함수 정의 ---\n",
    "def build_gt_for_eval(val_ids):\n",
    "    \"\"\"\n",
    "    pycocotools 평가를 위해, validation set에 대한 Ground Truth(실제 정답)를\n",
    "    COCO API가 요구하는 형식으로 변환합니다.\n",
    "    \"\"\"\n",
    "    gt_images = []\n",
    "    gt_annotations = []\n",
    "    ann_id_counter = 1\n",
    "\n",
    "    for img_id in val_ids:\n",
    "        img_info = img_meta[img_id]\n",
    "        gt_images.append({\n",
    "            'id': img_id,\n",
    "            'width': img_info['width'],\n",
    "            'height': img_info['height'],\n",
    "            'file_name': img_info['file_name']\n",
    "        })\n",
    "\n",
    "        for ann in ann_by_img.get(img_id, []):\n",
    "            gt_annotations.append({\n",
    "                'id': ann_id_counter,\n",
    "                'image_id': img_id,\n",
    "                'category_id': ann['category_id'],\n",
    "                'bbox': ann['bbox'],\n",
    "                'area': ann['bbox'][2] * ann['bbox'][3],\n",
    "                'iscrowd': 0\n",
    "            })\n",
    "            ann_id_counter += 1\n",
    "\n",
    "    return {\n",
    "        # --- (수정) 아래 두 줄 추가 ---\n",
    "        'info': {},      # info 키 추가 (내용은 비어도 됨)\n",
    "        'licenses': [],  # licenses 키 추가 (내용은 비어도 됨)\n",
    "        # --------------------------\n",
    "        'images': gt_images,\n",
    "        'annotations': gt_annotations,\n",
    "        'categories': COCO['categories']\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_for_eval(model, val_ids, device):\n",
    "    \"\"\"\n",
    "    Validation set 이미지들에 대해 예측을 수행하고,\n",
    "    pycocotools 형식에 맞는 결과 리스트를 반환합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for img_id in val_ids:\n",
    "        with Image.open(id2path[img_id]) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "        img_tensor = to_tensor(im).to(device)\n",
    "\n",
    "        outputs = model([img_tensor])[0]\n",
    "\n",
    "        boxes = outputs['boxes'].cpu().numpy()\n",
    "        labels = outputs['labels'].cpu().numpy() + 1 # 0-based -> 1-based\n",
    "        scores = outputs['scores'].cpu().numpy()\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if score < EVAL_SCORE_THR:\n",
    "                continue\n",
    "\n",
    "            box[2] -= box[0] # x2,y2 -> w,h\n",
    "            box[3] -= box[1]\n",
    "\n",
    "            predictions.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': box.tolist(),\n",
    "                'score': float(score)\n",
    "            })\n",
    "    return predictions\n",
    "\n",
    "def evaluate_map(model, val_ids, device):\n",
    "    \"\"\"mAP 평가를 총괄하는 메인 함수\"\"\"\n",
    "    if not val_ids:\n",
    "        print(\"[EVAL] 검증 데이터셋이 없어 mAP 평가를 건너뜁니다.\")\n",
    "        return None\n",
    "\n",
    "    # 1. Ground Truth(정답) 데이터 준비\n",
    "    gt_dataset = build_gt_for_eval(val_ids)\n",
    "    coco_gt = COCOapi()\n",
    "    coco_gt.dataset = gt_dataset\n",
    "    coco_gt.createIndex()\n",
    "\n",
    "    # 2. Prediction(모델 예측) 데이터 준비\n",
    "    pred_results = predict_for_eval(model, val_ids, device)\n",
    "    coco_dt = coco_gt.loadRes(pred_results)\n",
    "\n",
    "    # 3. pycocotools를 이용해 mAP 계산 및 출력\n",
    "    evaluator = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    evaluator.params.iouThrs = np.linspace(0.75, 0.95, 5) # 대회 평가지표\n",
    "    evaluator.evaluate()\n",
    "    evaluator.accumulate()\n",
    "    evaluator.summarize()\n",
    "\n",
    "    map_score = evaluator.stats[0]\n",
    "    print(f\"\\n[RESULT] 최종 mAP @[IoU=0.75:0.95] = {map_score:.4f}\")\n",
    "    return map_score\n",
    "\n",
    "def _parse_image_id_from_name(fname: str) -> int:\n",
    "    \"\"\"파일명에서 숫자 부분만 추출하는 헬퍼 함수\"\"\"\n",
    "    return int(\"\".join(filter(str.isdigit, Path(fname).stem)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_submission_csv(model, test_dir, out_csv, device):\n",
    "    \"\"\"테스트 이미지에 대한 예측 결과를 submission.csv 파일로 저장\"\"\"\n",
    "    if not test_dir.exists():\n",
    "        print(f\"[SUBMIT] 테스트 폴더({test_dir})가 없어 제출 파일 생성을 건너뜁니다.\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    results = []\n",
    "    ann_id_counter = 1\n",
    "    test_files = sorted(list(test_dir.glob(\"*.png\")) + list(test_dir.glob(\"*.jpg\")))\n",
    "\n",
    "    for img_path in test_files:\n",
    "        image_id = _parse_image_id_from_name(img_path.name)\n",
    "        with Image.open(img_path) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "        img_tensor = to_tensor(im).to(device)\n",
    "\n",
    "        outputs = model([img_tensor])[0]\n",
    "\n",
    "        boxes = outputs['boxes'].cpu().numpy()\n",
    "        labels = outputs['labels'].cpu().numpy() + 1 # 0-based -> 1-based\n",
    "        scores = outputs['scores'].cpu().numpy()\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if score < 0.05: continue\n",
    "\n",
    "            results.append([\n",
    "                ann_id_counter, image_id, int(label),\n",
    "                box[0], box[1], box[2] - box[0], box[3] - box[1], # xyxy -> xywh\n",
    "                score\n",
    "            ])\n",
    "            ann_id_counter += 1\n",
    "\n",
    "    # CSV 파일 작성\n",
    "    with open(out_csv, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"annotation_id\", \"image_id\", \"category_id\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\", \"score\"])\n",
    "        writer.writerows(results)\n",
    "    print(f\"\\n[SUBMIT] 제출 파일 생성 완료: {out_csv}\")\n",
    "    print(f\"  -> 총 {len(results)}개의 객체를 {len(test_files)}개의 이미지에서 예측했습니다.\")\n",
    "\n",
    "# --- 최종 실행 ---\n",
    "print(\"\\n--- mAP 평가 시작 ---\")\n",
    "stats = evaluate_map(model, val_ids, DEVICE)\n",
    "\n",
    "print(\"\\n--- 제출 파일 생성 시작 ---\")\n",
    "export_submission_csv(model, TEST_DIR, SUBMIT_CSV, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFdFN1jTDXua"
   },
   "source": [
    "## 섹터 11: 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1fYcFHITtjQNqGr5-DgeZskEH6D3-8z7m"
    },
    "executionInfo": {
     "elapsed": 12048,
     "status": "ok",
     "timestamp": 1757861143591,
     "user": {
      "displayName": "초코칩",
      "userId": "04966641623622524646"
     },
     "user_tz": -540
    },
    "id": "XZHTRFGPDYBk",
    "outputId": "4d8eb22b-bec3-4e5b-c911-cb86b35147bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================================\n",
    "# [섹터 11] 결과 시각화 (NameError 수정)\n",
    "# ================================================================\n",
    "# 폰트 설치 (Colab에서 한글 깨짐 방지, 이미 실행했다면 생략 가능)\n",
    "!apt-get -qq install -y fonts-nanum\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "# --- 시각화 설정값 ---\n",
    "SCORE_THR = 0.25      # 시각화할 예측의 최소 점수(confidence)\n",
    "TOPK_PER_IMAGE = 4    # 이미지당 최대 몇 개의 Bbox를 그릴지\n",
    "N_VIS = 8             # 총 몇 개의 이미지를 시각화할지\n",
    "VIS_DIR = ROOT / \"vis\"\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- (오류 수정) cat_id2name 변수 생성 ---\n",
    "# 이 셀을 단독으로 실행할 경우를 대비하여, 'cat_id2name' 변수를 여기서 다시 정의합니다.\n",
    "# 이 변수는 원래 [섹터 3]에서 생성됩니다.\n",
    "cat_id2name = {c[\"id\"]: c[\"name\"] for c in COCO[\"categories\"]}\n",
    "# -----------------------------------------\n",
    "\n",
    "\n",
    "# --- 함수 정의 ---\n",
    "def get_korean_font(size=15):\n",
    "    \"\"\"Colab에 설치된 나눔고딕 폰트를 가져오는 함수\"\"\"\n",
    "    font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "    try:\n",
    "        return ImageFont.truetype(font_path, size)\n",
    "    except IOError:\n",
    "        print(f\"[FONT] 나눔고딕 폰트를 찾을 수 없습니다. 기본 폰트를 사용합니다.\")\n",
    "        return ImageFont.load_default()\n",
    "\n",
    "def pretty_name(category_id: int):\n",
    "    \"\"\"\n",
    "    카테고리 ID를 입력받아, 가장 보기 좋은 형태의 알약 이름으로 변환합니다.\n",
    "    (예: '기넥신에프정(은행엽엑스)(수출용)' -> '기넥신에프정')\n",
    "    \"\"\"\n",
    "    if category_id in cat_id2name:\n",
    "        # 괄호와 그 안의 내용을 제거하여 이름을 간소화\n",
    "        name = cat_id2name[category_id]\n",
    "        return name.split('(')[0]\n",
    "    return f\"CLS_{category_id}\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_samples(model, image_ids):\n",
    "    \"\"\"주어진 이미지 ID들에 대해 모델 예측을 시각화하는 메인 함수\"\"\"\n",
    "    model.eval()\n",
    "    font = get_korean_font()\n",
    "\n",
    "    images_to_display = []\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        path = id2path.get(img_id)\n",
    "        if not path or not path.exists():\n",
    "            continue\n",
    "\n",
    "        # 1. 이미지 로드 및 추론 실행\n",
    "        with Image.open(path) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "        img_tensor = to_tensor(im).to(DEVICE)\n",
    "        outputs = model([img_tensor])[0]\n",
    "\n",
    "        # 2. 결과 필터링\n",
    "        boxes = outputs['boxes'].cpu().numpy()\n",
    "        labels = outputs['labels'].cpu().numpy() + 1 # 0-based -> 1-based\n",
    "        scores = outputs['scores'].cpu().numpy()\n",
    "\n",
    "        keep = scores >= SCORE_THR\n",
    "        boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
    "\n",
    "        if len(boxes) > TOPK_PER_IMAGE:\n",
    "            indices = np.argsort(-scores)[:TOPK_PER_IMAGE]\n",
    "            boxes, labels, scores = boxes[indices], labels[indices], scores[indices]\n",
    "\n",
    "        # 3. PIL을 이용해 Bbox와 라벨 그리기\n",
    "        vis_image = im.copy()\n",
    "        draw = ImageDraw.Draw(vis_image)\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            draw.rectangle(box.tolist(), outline=\"red\", width=3)\n",
    "            text = f\"{pretty_name(label)} {score:.2f}\"\n",
    "            text_bbox = draw.textbbox((box[0], box[1] - 18), text, font=font)\n",
    "            draw.rectangle(text_bbox, fill=\"red\")\n",
    "            draw.text((box[0], box[1] - 18), text, fill=\"white\", font=font)\n",
    "\n",
    "        # 4. 결과 저장 및 표시 리스트에 추가\n",
    "        save_path = VIS_DIR / f\"vis_{Path(path).name}\"\n",
    "        vis_image.save(save_path)\n",
    "\n",
    "        if len(images_to_display) < 4:\n",
    "             images_to_display.append(vis_image)\n",
    "\n",
    "    print(f\"\\n[VISUALIZE] 시각화 결과 {len(image_ids)}개를 {VIS_DIR} 폴더에 저장했습니다.\")\n",
    "\n",
    "    for img in images_to_display:\n",
    "        display(img)\n",
    "\n",
    "# --- 최종 실행 ---\n",
    "if val_ids:\n",
    "    sample_ids = random.sample(val_ids, min(N_VIS, len(val_ids)))\n",
    "    print(f\"{len(sample_ids)}개의 검증 이미지에 대한 예측을 시각화합니다...\")\n",
    "    visualize_samples(model, sample_ids)\n",
    "else:\n",
    "    print(\"시각화할 검증 이미지가 없습니다.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOv4SGQGiV1bNrOh2IpmpbN",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
